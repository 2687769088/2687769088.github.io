---
title: CUDA矩阵乘法的优化
tags:
  - 高性能计算
---

使用下面一种或多种优化方法完成 CUDA 的矩阵乘法 $C\to\alpha AB+\beta C$

- 使用 global memory 合并访存
- 采用分块乘法，使用 shared memory
- 请找出最佳的执行配置参数：grid 和 block

其中 $A,B,C$ 均为 $2^{14}\times 2^{14}$的方阵。

## 实验环境

实验在 TH-2K 的一个节点上进行。单节点的显卡配置如下：

```bash
$ nvidia-smi
Tue Dec 13 20:51:41 2020
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 418.67       Driver Version: 418.67       CUDA Version: 10.1     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla V100-SXM2...  Off  | 00000000:8A:00.0 Off |                    0 |
| N/A   32C    P0    37W / 300W |      0MiB / 16130MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  Tesla V100-SXM2...  Off  | 00000000:8B:00.0 Off |                    0 |
| N/A   31C    P0    38W / 300W |      0MiB / 16130MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   2  Tesla V100-SXM2...  Off  | 00000000:B3:00.0 Off |                    0 |
| N/A   31C    P0    37W / 300W |      0MiB / 16130MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   3  Tesla V100-SXM2...  Off  | 00000000:B4:00.0 Off |                    0 |
| N/A   31C    P0    37W / 300W |      0MiB / 16130MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
```

## 实验原理

优化 CUDA 架构上的程序，一般从以下几个方面考虑：

- 选择好的并行算法，发掘更多的数据并行性
- 保持 SM 尽可能忙碌，尽量利用所有的 SM 参与计算
  - 加大数据量
  - 减小线程块大小
- 优化存储器的使用
  - 全局存储器合并访问
  - 使用更快的 constant memory 或 shared memory

## 实验过程

> - 使用 global memory 合并访存
> - 采用分块乘法，使用 shared memory
> - 请找出最佳的执行配置参数：grid 和 blocks

这次实验和上一个实验[CUDA 矩阵向量乘的多种优化](https://wu-kan.cn/_posts/2019-11-29-CUDA%E7%9F%A9%E9%98%B5%E5%90%91%E9%87%8F%E4%B9%98%E7%9A%84%E5%A4%9A%E7%A7%8D%E4%BC%98%E5%8C%96/)其实非常相似：矩阵向量乘法中一个线程对应答案向量中的一个元素，矩阵矩阵乘法中一个线程对应答案矩阵的一个元素（使用二维分块）。不过，还有这些代码细节需要注意：

- 矩阵 B 的访问天然就是连续的，要使得对矩阵 A 的访问连续，可以考虑在 A 的列优先表达上进行计算；
- 矩阵 A 和 B 的每个位置都被访问了多次，因此都可以使用 shared memory 进行优化
- 由于测试的矩阵是长宽相等的正方形，因此在两个维度上的计算是大抵相似的，分块时也按正方形分块会有最少的访存（global 内存）次数
- 一个线程块里最多 1024 个线程，这里由于要正方形分块，因此分块的宽度不能超过 32（$32\times 32=1024$）
- 二维 Shared Memory 大小增加了一位用于避免列维度上多个线程访问同一个 Bank 产生 Bank conflict（不妨`#define double float`，更加明显，因为现在的卡的 bank 至少都有 32 个 banks，每个 bank 带宽是 32bit）

## 源代码

### `gemm.cu`

```cpp
#include <stdio.h>
#include <functional>
#include <cublas_v2.h>
#include <cuda_runtime.h>
#include <thrust/device_vector.h>
#define IDX2C(i, j, ld) (((j) * (ld)) + (i))

namespace Alcanderian //https://github.com/Alcanderian/CUDA-tutorial/sgemm
{
    __global__ void cuda_kernel_sgemm_1(
        float *a,
        float *b,
        float *c,
        size_t N,
        size_t M,
        size_t K,
        float alpha,
        float beta)
    {
        int tr = threadIdx.x;                   // row idx in block
        int tc = threadIdx.y;                   // col idx in block
        int ir = blockIdx.x * 32 + threadIdx.x; // row idx in global
        int ic = blockIdx.y * 32 + threadIdx.y; // col idx in global

        __shared__ float a_sub[32][32];
        __shared__ float b_sub[32][32];

        int load_size = K / 32;
        if (K % 32 != 0)
        {
            load_size += 1;
        }
        float acc = 0.0f;
        int a_ir = ir;
        int b_ic = ic;
#define idx(ri, ci, nc) ((ri) * (nc) + (ci))
        for (int l = 0; l < load_size; ++l)
        {
            int a_ic = l * 32 + tc;
            int b_ir = l * 32 + tr;
            a_sub[tr][tc] = 0.0f;
            b_sub[tr][tc] = 0.0f;
            if (a_ir < M && a_ic < K)
                a_sub[tr][tc] = a[idx(a_ir, a_ic, K)];
            if (b_ir < K && b_ic < N)
                b_sub[tr][tc] = b[idx(b_ir, b_ic, N)];

            __syncthreads();

#pragma unroll
            for (int k = 0; k < 32; ++k)
            {
                acc += a_sub[tr][k] * b_sub[k][tc];
            }

            __syncthreads();
        }

        if (ir < M && ic < N)
            c[idx(ir, ic, N)] = alpha * acc + beta * c[idx(ir, ic, N)];
#undef idx
    }

    // use __ldg & avoid bank conflict
    __global__ void cuda_kernel_sgemm_2(float *__restrict__ a,
                                        float *__restrict__ b,
                                        float *__restrict__ c,
                                        size_t N,
                                        size_t M,
                                        size_t K,
                                        float alpha,
                                        float beta)
    {
        int tr = threadIdx.x;                   // row idx in block
        int tc = threadIdx.y;                   // col idx in block
        int ir = blockIdx.x * 32 + threadIdx.x; // row idx in global
        int ic = blockIdx.y * 32 + threadIdx.y; // col idx in global

        __shared__ float a_sub[32][32 + 1]; // avoid bank conflict
        __shared__ float b_sub[32][32 + 1];

        int load_size = K / 32;
        if (K % 32 != 0)
        {
            load_size += 1;
        }
        float acc = 0.0f;
        int a_ir = ir;
        int b_ic = ic;
#define idx(ri, ci, nc) ((ri) * (nc) + (ci))
        for (int l = 0; l < load_size; ++l)
        {
            int a_ic = l * 32 + tc;
            int b_ir = l * 32 + tr;
            a_sub[tr][tc] = 0.0f;
            b_sub[tr][tc] = 0.0f;
            if (a_ir < M && a_ic < K)
                a_sub[tr][tc] = a[idx(a_ir, a_ic, K)];
            if (b_ir < K && b_ic < N)
                b_sub[tr][tc] = b[idx(b_ir, b_ic, N)];

            __syncthreads();

#pragma unroll
            for (int k = 0; k < 32; ++k)
            {
                acc += a_sub[tr][k] * b_sub[k][tc];
            }

            __syncthreads();
        }

        if (ir < M && ic < N)
            c[idx(ir, ic, N)] = alpha * acc + beta * c[idx(ir, ic, N)];
#undef idx
    }

    // use __ldg without avoiding bank conflict
    __global__ void cuda_kernel_sgemm_3(
        float *__restrict__ a,
        float *__restrict__ b,
        float *__restrict__ c,
        size_t N,
        size_t M,
        size_t K,
        float alpha,
        float beta)
    {
        int tr = threadIdx.x;                   // row idx in block
        int tc = threadIdx.y;                   // col idx in block
        int ir = blockIdx.x * 32 + threadIdx.x; // row idx in global
        int ic = blockIdx.y * 32 + threadIdx.y; // col idx in global

        __shared__ float a_sub[32][32]; // avoid bank conflict
        __shared__ float b_sub[32][32];

        int load_size = K / 32;
        if (K % 32 != 0)
        {
            load_size += 1;
        }
        float acc = 0.0f;
        int a_ir = ir;
        int b_ic = ic;
#define idx(ri, ci, nc) ((ri) * (nc) + (ci))
        for (int l = 0; l < load_size; ++l)
        {
            int a_ic = l * 32 + tc;
            int b_ir = l * 32 + tr;
            a_sub[tr][tc] = 0.0f;
            b_sub[tr][tc] = 0.0f;
            if (a_ir < M && a_ic < K)
                a_sub[tr][tc] = a[idx(a_ir, a_ic, K)];
            if (b_ir < K && b_ic < N)
                b_sub[tr][tc] = b[idx(b_ir, b_ic, N)];

            __syncthreads();

#pragma unroll
            for (int k = 0; k < 32; ++k)
            {
                acc += a_sub[tr][k] * b_sub[k][tc];
            }

            __syncthreads();
        }

        if (ir < M && ic < N)
            c[idx(ir, ic, N)] = alpha * acc + beta * c[idx(ir, ic, N)];
#undef idx
    }
}; // namespace Alcanderian

namespace fynv //https://github.com/fynv/optimal_sgemm_cuda_c
{
    struct f8
    {
        float4 a, b;
        __device__ inline f8()
        {
            memset(this, 0, sizeof(f8));
        }
    };

    struct f88
    {
        f8 a, b, c, d, e, f, g, h;
    };

    __device__ inline void d_load8(const float *p, f8 &c)
    {
        c.a = ((float4 *)p)[0];
        c.b = ((float4 *)p)[16];
    }

    __device__ inline void d_store8(float *p, const f8 &c)
    {
        ((float4 *)p)[0] = c.a;
        ((float4 *)p)[16] = c.b;
    }

    __device__ inline void d_mult8v(f8 &c, const f8 &a, float b)
    {
        c.a.x += a.a.x * b;
        c.a.y += a.a.y * b;
        c.a.z += a.a.z * b;
        c.a.w += a.a.w * b;
        c.b.x += a.b.x * b;
        c.b.y += a.b.y * b;
        c.b.z += a.b.z * b;
        c.b.w += a.b.w * b;
    }

    template <typename T>
    __device__ inline void Swap(T &a, T &b)
    {
        T t = a;
        a = b;
        b = t;
    }

    __global__ __launch_bounds__(256) //https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#launch-bounds
        void g_fgemm(
            float *d_C,
            const float *d_A,
            const float *d_B,
            int n,
            int lda,
            int ldb,
            int ldc)
    {
        int x_a = threadIdx.x & 31;
        int y_a = threadIdx.x >> 5;

        int x_b = threadIdx.x & 1;
        int y_b = threadIdx.x >> 1;

        int x_c = threadIdx.x & 15;
        int y_c = threadIdx.x >> 4;

        __shared__ float smem[4096];
        float *s_A1 = smem;
        float *s_A2 = smem + 1024;
        float *s_B1 = smem + 2048;
        float *s_B2 = smem + 3072;

        f88 l_C;

        const float *p_A = d_A + (blockIdx.x << 7);
        const float *p_B = d_B + (blockIdx.y << 7) * ldb;

        float4 p, q;
        p = ((float4 *)p_A)[y_a * (lda >> 2) + x_a];
        q = ((float4 *)p_B)[y_b * (ldb >> 2) + x_b];

        for (int i = 0; i < n; i += 8)
        {
            ((float4 *)s_A1)[threadIdx.x] = p;
            s_B1[(((x_b << 2) + 0) << 7) + y_b] = q.x;
            s_B1[(((x_b << 2) + 1) << 7) + y_b] = q.y;
            s_B1[(((x_b << 2) + 2) << 7) + y_b] = q.z;
            s_B1[(((x_b << 2) + 3) << 7) + y_b] = q.w;
            __syncthreads();

            if (i + 8 < n)
            {
                p_A += (lda << 3);
                p_B += 8;
                p = ((float4 *)p_A)[y_a * (lda >> 2) + x_a];
                q = ((float4 *)p_B)[y_b * (ldb >> 2) + x_b];
            }

            for (int j = 0; j < 8; j++)
            {
                float *p_s_A = s_A1 + (j << 7) + (x_c << 2);
                float *p_s_B = s_B1 + (j << 7) + (y_c << 2);

                f8 a, b;
                d_load8(p_s_A, a);
                d_load8(p_s_B, b);

                d_mult8v(l_C.a, a, b.a.x);
                d_mult8v(l_C.b, a, b.a.y);
                d_mult8v(l_C.c, a, b.a.z);
                d_mult8v(l_C.d, a, b.a.w);
                d_mult8v(l_C.e, a, b.b.x);
                d_mult8v(l_C.f, a, b.b.y);
                d_mult8v(l_C.g, a, b.b.z);
                d_mult8v(l_C.h, a, b.b.w);
            }

            Swap(s_A1, s_A2);
            Swap(s_B1, s_B2);
        }

        float *p_C = d_C + ((blockIdx.x << 7) + (x_c << 2)) + ((blockIdx.y << 7) + (y_c << 2)) * ldc;
        d_store8(p_C, l_C.a);
        p_C += ldc;
        d_store8(p_C, l_C.b);
        p_C += ldc;
        d_store8(p_C, l_C.c);
        p_C += ldc;
        d_store8(p_C, l_C.d);
        p_C += (ldc * 61);
        d_store8(p_C, l_C.e);
        p_C += ldc;
        d_store8(p_C, l_C.f);
        p_C += ldc;
        d_store8(p_C, l_C.g);
        p_C += ldc;
        d_store8(p_C, l_C.h);
    }
}; // namespace fynv

namespace wuk
{
    template <typename T>
    __global__ void
    gemm_32x32_v0(
        int m,
        int n,
        int k,
        T alpha,
        const T *A,
        int ldA,
        const T *B,
        int ldB,
        T beta,
        T *C,
        int ldC)
    {
        for (int global_x = blockIdx.x * blockDim.x + threadIdx.x; global_x < m;
             global_x += gridDim.x * blockDim.x)
            for (int global_y = blockIdx.y * blockDim.y + threadIdx.y; global_y < n;
                 global_y += gridDim.y * blockDim.y)
            {
                T resC = 0;
                for (int i = 0; i < k; ++i)
                    resC += A[IDX2C(global_y, i, ldA)] * B[IDX2C(i, global_x, ldB)];
                resC = resC * alpha + C[IDX2C(global_y, global_x, ldC)] * beta;
                C[IDX2C(global_y, global_x, ldC)] = resC;
            }
    }

    template <typename T>
    __global__ void
    gemm_32x32_v1(
        int m,
        int n,
        int k,
        T alpha,
        const T *A,
        int ldA,
        const T *B,
        int ldB,
        T beta,
        T *C,
        int ldC)
    {
        for (int global_x = blockIdx.x * blockDim.x + threadIdx.x; global_x < m;
             global_x += gridDim.x * blockDim.x)
            for (int global_y = blockIdx.y * blockDim.y + threadIdx.y; global_y < n;
                 global_y += gridDim.y * blockDim.y)
            {
                T resC = 0;
                for (int i = 0; i < k; ++i)
                {
                    const T resA = A[IDX2C(global_x, i, ldA)];
                    const T resB = B[IDX2C(i, global_y, ldB)];
                    resC += resA * resB;
                }
                resC = resC * alpha + C[IDX2C(global_x, global_y, ldC)] * beta;
                C[IDX2C(global_x, global_y, ldC)] = resC;
            }
    }

    template <typename T, int TILE_WIDTH>
    __global__ void
    gemm_32x32_v2(
        int m,
        int n,
        int k,
        T alpha,
        const T *A,
        int ldA,
        const T *B,
        int ldB,
        T beta,
        T *C,
        int ldC)
    {
        for (int global_x = blockIdx.x * blockDim.x + threadIdx.x; global_x < m;
             global_x += gridDim.x * blockDim.x)
            for (int global_y = blockIdx.y * blockDim.y + threadIdx.y; global_y < n;
                 global_y += gridDim.y * blockDim.y)
            {
                T resC = 0;
                __shared__ T sA[TILE_WIDTH][TILE_WIDTH];
                __shared__ T sB[TILE_WIDTH][TILE_WIDTH];
                for (int i = 0; i < k; i += TILE_WIDTH)
                {
                    __syncthreads();
                    sA[threadIdx.x][threadIdx.y] =
                        A[IDX2C(global_x, i + threadIdx.y, ldA)];
                    sB[threadIdx.x][threadIdx.y] =
                        B[IDX2C(i + threadIdx.x, global_y, ldB)];
                    __syncthreads();
                    for (int j = 0; j < TILE_WIDTH; ++j)
                        resC += sA[threadIdx.x][j] * sB[j][threadIdx.y];
                }
                if (global_y < m && global_x < n)
                {
                    resC = resC * alpha + C[IDX2C(global_x, global_y, ldC)] * beta;
                    C[IDX2C(global_x, global_y, ldC)] = resC;
                }
            }
    }

    template <typename T, int TILE_WIDTH>
    __global__ void
    gemm_32x32_v3(
        int m,
        int n,
        int k,
        T alpha,
        const T *A,
        int ldA,
        const T *B,
        int ldB,
        T beta,
        T *C,
        int ldC)
    {
        for (int global_x = blockIdx.x * blockDim.x + threadIdx.x; global_x < m;
             global_x += gridDim.x * blockDim.x)
            for (int global_y = blockIdx.y * blockDim.y + threadIdx.y; global_y < n;
                 global_y += gridDim.y * blockDim.y)
            {
                T resC = 0;
                T __shared__ sA[TILE_WIDTH][TILE_WIDTH | 1];
                T __shared__ sB[TILE_WIDTH][TILE_WIDTH | 1];
                for (int i = 0; i < k; i += TILE_WIDTH)
                {
                    __syncthreads();
                    sA[threadIdx.x][threadIdx.y] =
                        A[IDX2C(global_x, i + threadIdx.y, ldA)];
                    sB[threadIdx.x][threadIdx.y] =
                        B[IDX2C(i + threadIdx.x, global_y, ldB)];
                    __syncthreads();
                    for (int j = 0; j < TILE_WIDTH; ++j)
                        resC += sA[threadIdx.x][j] * sB[j][threadIdx.y];
                }
                if (global_y < m && global_x < n)
                {
                    resC = resC * alpha + C[IDX2C(global_x, global_y, ldC)] * beta;
                    C[IDX2C(global_x, global_y, ldC)] = resC;
                }
            }
    }
    __global__ __launch_bounds__(256) //https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#launch-bounds
        void sgemm_128x128(
            float *d_C,
            const float *d_A,
            const float *d_B,
            int n,
            int lda,
            int ldb,
            int ldc)
    {
        struct Float8
        {
            float4 v[2];
            __device__ __forceinline__ Float8()
            {
                memset(this, 0, sizeof(*this));
            }
            __device__ __forceinline__ Float8(const float *p)
            {
                v[0] = ((float4 *)p)[0];
                v[1] = ((float4 *)p)[16];
            }
            __device__ __forceinline__ void write(float *p)
            {
                ((float4 *)p)[0] = v[0];
                ((float4 *)p)[16] = v[1];
            }
            __device__ __forceinline__ void axpy(float alpha, const Float8 &x)
            {
#pragma unroll
                for (int i = 0; i < 2; ++i)
                {
                    v[i].x += x.v[i].x * alpha;
                    v[i].y += x.v[i].y * alpha;
                    v[i].z += x.v[i].z * alpha;
                    v[i].w += x.v[i].w * alpha;
                }
            }
        } r_C[8];

        int x_a = threadIdx.x & 31;
        int y_a = threadIdx.x >> 5;

        int x_b = threadIdx.x & 1;
        int y_b = threadIdx.x >> 1;

        int x_c = threadIdx.x & 15;
        int y_c = threadIdx.x >> 4;

        __shared__ float s_buffer[2048];
        float *s_A = s_buffer;
        float *s_B = s_buffer + 1024;

#define TWO_BUFFER
#ifdef TWO_BUFFER
        __shared__ float s_tbuffer[2048];
        float *s_tA = s_tbuffer;
        float *s_tB = s_tbuffer + 1024;
#endif

        const float *p_A = d_A + (blockIdx.x << 7);
        const float *p_B = d_B + (blockIdx.y << 7) * ldb;

        float4 r_A = ((float4 *)p_A)[x_a + y_a * (lda >> 2)];
        float4 r_B = ((float4 *)p_B)[x_b + y_b * (ldb >> 2)];

        for (int i = 0; i < n; i += 8)
        {
            ((float4 *)s_A)[threadIdx.x] = r_A;
            s_B[(((x_b << 2) + 0) << 7) + y_b] = r_B.x;
            s_B[(((x_b << 2) + 1) << 7) + y_b] = r_B.y;
            s_B[(((x_b << 2) + 2) << 7) + y_b] = r_B.z;
            s_B[(((x_b << 2) + 3) << 7) + y_b] = r_B.w;
            __syncthreads();

            if (i + 8 < n)
            {
                p_A += lda << 3;
                p_B += 8;
                r_A = ((float4 *)p_A)[x_a + y_a * (lda >> 2)];
                r_B = ((float4 *)p_B)[x_b + y_b * (ldb >> 2)];
            }

#pragma unroll(4)
            for (int j = 0; j < 8; ++j)
            {
                const Float8
                    a(s_A + (j << 7) + (x_c << 2)),
                    b(s_B + (j << 7) + (y_c << 2));
#pragma unroll
                for (int k = 0; k < 2; ++k)
                {
                    r_C[(k << 2) + 0].axpy(b.v[k].x, a);
                    r_C[(k << 2) + 1].axpy(b.v[k].y, a);
                    r_C[(k << 2) + 2].axpy(b.v[k].z, a);
                    r_C[(k << 2) + 3].axpy(b.v[k].w, a);
                }
            }

#ifdef TWO_BUFFER
            {
                float *tmp_A = s_A;
                s_A = s_tA;
                s_tA = tmp_A;
            }
            {
                float *tmp_B = s_B;
                s_B = s_tB;
                s_tB = tmp_B;
            }
#else
            __syncthreads();
#endif
        }

        float *p_C = d_C + ((blockIdx.x << 7) + (x_c << 2)) + ((blockIdx.y << 7) + (y_c << 2)) * ldc;
        r_C[0].write(p_C);
        r_C[1].write(p_C + ldc);
        r_C[2].write(p_C + ldc * 2);
        r_C[3].write(p_C + ldc * 3);
        r_C[4].write(p_C + ldc * 64);
        r_C[5].write(p_C + ldc * 65);
        r_C[6].write(p_C + ldc * 66);
        r_C[7].write(p_C + ldc * 67);
    }
}; // namespace wuk

//https://github.com/navdeepkk/gpu-gemms
//https://github.com/wjc404/Simple_CUDA_GEMM

struct WuK_Timer
{
    WuK_Timer(const char *tag, float flo, const std::function<void()> &kernel, int test_time = 10)
    {
        float min_time = 9e99;
        while (test_time--)
        {
            cudaEvent_t beg, end;
            cudaEventCreate(&beg);
            cudaEventCreate(&end);
            cudaEventRecord(beg);
            kernel();
            cudaEventRecord(end);
            cudaEventSynchronize(beg);
            cudaEventSynchronize(end);
            float elapsed_time;
            cudaEventElapsedTime(&elapsed_time, beg, end);
            min_time = std::min(min_time, elapsed_time);
        }
        printf("%s: %f ms, %e FLOPS.\n", tag, min_time, flo * 1e3 / min_time);
    }
};
struct WuK_cublas
{
    cublasHandle_t handle;
    WuK_cublas() { cublasCreate(&handle); }
    ~WuK_cublas() { cublasDestroy(handle); }
} wuk_cublas;
const float alpha = 2, beta = 3;
const cublasOperation_t opA = CUBLAS_OP_N, opB = CUBLAS_OP_N, opC = CUBLAS_OP_N;
const int m = 1 << 13, n = 1 << 13, k = 1 << 13,
          ldA = opA == CUBLAS_OP_N ? k : m, ldB = opB == CUBLAS_OP_N ? n : k,
          ldC = opC == CUBLAS_OP_N ? n : m;
thrust::device_vector<float> dA(m *k, 1), dB(k *n, 1), dC(m *n, 0);

int main()
{
    WuK_Timer(
        "cublasSgemm", 2.0 * m * k * n,
        [&] {
            cublasSgemm(
                wuk_cublas.handle, opA, opB, m, n, k, &alpha,
                thrust::raw_pointer_cast(dA.data()), ldA,
                thrust::raw_pointer_cast(dB.data()), ldB, &beta,
                thrust::raw_pointer_cast(dC.data()), ldC);
        });
    WuK_Timer(
        "fynv::g_fgemm", 2.0 * m * k * n,
        [&] {
            const int TILE_WIDTH = 128;
            const dim3 blockDim(256),
                gridDim(n / TILE_WIDTH, n / TILE_WIDTH);
            assert(opA == CUBLAS_OP_N);
            assert(opB == CUBLAS_OP_N);
            assert(opC == CUBLAS_OP_N);
            assert(m == n);
            assert(k == n);
            assert(n % TILE_WIDTH == 0);
            fynv::g_fgemm<<<gridDim, blockDim>>>(
                thrust::raw_pointer_cast(dC.data()),
                thrust::raw_pointer_cast(dA.data()),
                thrust::raw_pointer_cast(dB.data()),
                n, ldA, ldB, ldC);
        });
    WuK_Timer(
        "Alcanderian::cuda_kernel_sgemm_1", 2.0 * m * k * n,
        [&] {
            const int TILE_WIDTH = 32;
            const dim3 blockDim(32, 32),
                gridDim((m + blockDim.x - 1) / blockDim.x,
                        (n + blockDim.y - 1) / blockDim.y);
            assert(opA == CUBLAS_OP_N);
            assert(opB == CUBLAS_OP_N);
            assert(opC == CUBLAS_OP_N);
            assert(m % TILE_WIDTH == 0);
            assert(n % TILE_WIDTH == 0);
            assert(k % TILE_WIDTH == 0);
            assert(TILE_WIDTH == blockDim.x);
            assert(TILE_WIDTH == blockDim.y);
            Alcanderian::cuda_kernel_sgemm_1<<<gridDim, blockDim>>>(
                thrust::raw_pointer_cast(dA.data()),
                thrust::raw_pointer_cast(dB.data()),
                thrust::raw_pointer_cast(dC.data()), n, m, k, alpha, beta);
        });
    WuK_Timer(
        "Alcanderian::cuda_kernel_sgemm_2", 2.0 * m * k * n,
        [&] {
            const int TILE_WIDTH = 32;
            const dim3 blockDim(32, 32),
                gridDim((m + blockDim.x - 1) / blockDim.x,
                        (n + blockDim.y - 1) / blockDim.y);
            assert(opA == CUBLAS_OP_N);
            assert(opB == CUBLAS_OP_N);
            assert(opC == CUBLAS_OP_N);
            assert(m % TILE_WIDTH == 0);
            assert(n % TILE_WIDTH == 0);
            assert(k % TILE_WIDTH == 0);
            assert(TILE_WIDTH == blockDim.x);
            assert(TILE_WIDTH == blockDim.y);
            Alcanderian::cuda_kernel_sgemm_2<<<gridDim, blockDim>>>(
                thrust::raw_pointer_cast(dA.data()),
                thrust::raw_pointer_cast(dB.data()),
                thrust::raw_pointer_cast(dC.data()), n, m, k, alpha, beta);
        });
    WuK_Timer(
        "Alcanderian::cuda_kernel_sgemm_3", 2.0 * m * k * n,
        [&] {
            const int TILE_WIDTH = 32;
            const dim3 blockDim(32, 32),
                gridDim((m + blockDim.x - 1) / blockDim.x,
                        (n + blockDim.y - 1) / blockDim.y);
            assert(opA == CUBLAS_OP_N);
            assert(opB == CUBLAS_OP_N);
            assert(opC == CUBLAS_OP_N);
            assert(m % TILE_WIDTH == 0);
            assert(n % TILE_WIDTH == 0);
            assert(k % TILE_WIDTH == 0);
            assert(TILE_WIDTH == blockDim.x);
            assert(TILE_WIDTH == blockDim.y);
            Alcanderian::cuda_kernel_sgemm_3<<<gridDim, blockDim>>>(
                thrust::raw_pointer_cast(dA.data()),
                thrust::raw_pointer_cast(dB.data()),
                thrust::raw_pointer_cast(dC.data()), n, m, k, alpha, beta);
        });
    WuK_Timer(
        "wuk::gemm_32x32_v0", 2.0 * m * k * n,
        [&] {
            const dim3 blockDim(32, 32),
                gridDim((m + blockDim.x - 1) / blockDim.x,
                        (n + blockDim.y - 1) / blockDim.y);
            assert(opA == CUBLAS_OP_N);
            assert(opB == CUBLAS_OP_N);
            assert(opC == CUBLAS_OP_N);
            wuk::gemm_32x32_v0<float><<<gridDim, blockDim>>>(
                m, n, k, alpha, thrust::raw_pointer_cast(dA.data()), ldA,
                thrust::raw_pointer_cast(dB.data()), ldB, beta,
                thrust::raw_pointer_cast(dC.data()), ldC);
        });
    WuK_Timer(
        "wuk::gemm_32x32_v1", 2.0 * m * k * n,
        [&] {
            const dim3 blockDim(32, 32),
                gridDim((m + blockDim.x - 1) / blockDim.x,
                        (n + blockDim.y - 1) / blockDim.y);
            assert(opA == CUBLAS_OP_N);
            assert(opB == CUBLAS_OP_N);
            assert(opC == CUBLAS_OP_N);
            wuk::gemm_32x32_v1<float><<<gridDim, blockDim>>>(
                m, n, k, alpha, thrust::raw_pointer_cast(dA.data()), ldA,
                thrust::raw_pointer_cast(dB.data()), ldB, beta,
                thrust::raw_pointer_cast(dC.data()), ldC);
        });
    WuK_Timer(
        "wuk::gemm_32x32_v2", 2.0 * m * k * n,
        [&] {
            const int TILE_WIDTH = 32;
            const dim3 blockDim(32, 32),
                gridDim((m + blockDim.x - 1) / blockDim.x,
                        (n + blockDim.y - 1) / blockDim.y);
            assert(opA == CUBLAS_OP_N);
            assert(opB == CUBLAS_OP_N);
            assert(opC == CUBLAS_OP_N);
            assert(m % TILE_WIDTH == 0);
            assert(n % TILE_WIDTH == 0);
            assert(k % TILE_WIDTH == 0);
            assert(TILE_WIDTH == blockDim.x);
            assert(TILE_WIDTH == blockDim.y);
            wuk::gemm_32x32_v2<float, TILE_WIDTH><<<gridDim, blockDim>>>(
                m, n, k, alpha, thrust::raw_pointer_cast(dA.data()), ldA,
                thrust::raw_pointer_cast(dB.data()), ldB, beta,
                thrust::raw_pointer_cast(dC.data()), ldC);
        });
    WuK_Timer(
        "wuk::gemm_32x32_v3", 2.0 * m * k * n,
        [&] {
            const int TILE_WIDTH = 32;
            const dim3 blockDim(32, 32),
                gridDim((m + blockDim.x - 1) / blockDim.x,
                        (n + blockDim.y - 1) / blockDim.y);
            assert(opA == CUBLAS_OP_N);
            assert(opB == CUBLAS_OP_N);
            assert(opC == CUBLAS_OP_N);
            assert(m % TILE_WIDTH == 0);
            assert(n % TILE_WIDTH == 0);
            assert(k % TILE_WIDTH == 0);
            assert(TILE_WIDTH == blockDim.x);
            assert(TILE_WIDTH == blockDim.y);
            wuk::gemm_32x32_v3<float, TILE_WIDTH><<<gridDim, blockDim>>>(
                m, n, k, alpha, thrust::raw_pointer_cast(dA.data()), ldA,
                thrust::raw_pointer_cast(dB.data()), ldB, beta,
                thrust::raw_pointer_cast(dC.data()), ldC);
        });
    WuK_Timer(
        "wuk::sgemm_128x128", 2.0 * m * k * n,
        [&] {
            const int TILE_WIDTH = 128;
            const dim3 blockDim(256),
                gridDim(n / TILE_WIDTH, n / TILE_WIDTH);
            assert(opA == CUBLAS_OP_N);
            assert(opB == CUBLAS_OP_N);
            assert(opC == CUBLAS_OP_N);
            assert(m == n);
            assert(k == n);
            assert(n % TILE_WIDTH == 0);
            wuk::sgemm_128x128<<<gridDim, blockDim>>>(
                thrust::raw_pointer_cast(dC.data()),
                thrust::raw_pointer_cast(dA.data()),
                thrust::raw_pointer_cast(dB.data()),
                n, ldA, ldB, ldC);
        });
}
```

### `slurm-o667552.out`

```bash
cublasSgemm: 72.147522 ms, 1.523977e+13 FLOPS.
fynv::g_fgemm: 77.569054 ms, 1.417462e+13 FLOPS.
Alcanderian::cuda_kernel_sgemm_1: 2119.572021 ms, 5.187423e+11 FLOPS.
Alcanderian::cuda_kernel_sgemm_2: 614.058594 ms, 1.790565e+12 FLOPS.
Alcanderian::cuda_kernel_sgemm_3: 2119.480957 ms, 5.187646e+11 FLOPS.
wuk::gemm_32x32_v0: 4666.458008 ms, 2.356202e+11 FLOPS.
wuk::gemm_32x32_v1: 470.199799 ms, 2.338392e+12 FLOPS.
wuk::gemm_32x32_v2: 1565.851318 ms, 7.021814e+11 FLOPS.
wuk::gemm_32x32_v3: 325.082825 ms, 3.382251e+12 FLOPS.
wuk::sgemm_128x128: 76.159393 ms, 1.443698e+13 FLOPS.
```

### `gemm.th2k.slurm`

```bash
#!/bin/bash
#SBATCH -J WuK
#SBATCH -p gpu_v100
#SBATCH -N 1
#SBATCH --exclusive

DIR=/GPUFS/sysu_hpcedu_302/asc20/WuK
cd $DIR

source spack-0.16.0/share/spack/setup-env.sh
spack load cuda@10.1.243
spack load gcc@7.5.0

nvcc gemm.cu -o gemm -run -lcublas -arch=compute_70 -code=sm_70 -O2 -use_fast_math -Xcompiler -Ofast
rm gemm
```
